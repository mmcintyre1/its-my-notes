---
last_modified_date: "2022-07-22 19:40:54.029576"
nav_order: 0
book_title: "Data Mesh: Delivering Data-Driven Value at Scale"
author: "Zhamak Dehghani"
publication_year: 2022
---
# Data Mesh: Delivering Data-Driven Value at Scale
{: .no_toc }

<details open markdown="block">
  <summary>
    Table of contents
  </summary>
1. TOC
{:toc}
</details>


## What is Data Mesh?
- Data mesh is a decentralized sociotechnical approach to share, access, and manage analytical data in complex and large-scale environments—within or across organizations
- data mesh looks to achieve these outcomes:
  - Respond gracefully to change: a business’s essential complexity, volatility, and uncertainty
  - Sustain agility in the face of growth
  - Increase the ratio of value from data to investment1

organizational shifts required:
<div style="text-align:center">
  <a href="/assets/img/data-mesh-shifts.jpg">
    <img src="/assets/img/data-mesh-shifts.jpg" alt="Shifts required">
  </a>
</div>

## Data Principles
### Domain-Oriented Ownership
- underpinned by domain-driven design to find seams of organizational units to decompose data
- **domain** is a sphere of knowledge, influence, or activity
- **bounded context** - delimited applicability of a particular model that gives team members a clear and shared understanding of what has to be consistent and what can develop independently
- **context mapping** - define relationship between bounded contexts/independent models

three types of domain data
1. **Source-aligned domain data**
  - Analytical data reflecting the business facts generated by the operational systems (native data product)
2. **Aggregate domain data**
  - Analytical data that is an aggregate of multiple upstream domains.
3. **Consumer-aligned domain data**
  - Analytical data transformed to fit the needs of one or multiple specific use cases (fit-for-purpose domain data)

{: .warn }
> Exposing analytical data directly from the operational database is an antipattern

- work with multiple models of shared entities rather than single source of truth, which is expensive and impediment to scale
- In data mesh, a data pipeline is simply an internal implementation of the data domain and is handled internally within the domain. It’s an implementation detail that must be abstracted from outside of the domain (26)

### Data as a Product

- successful products have three common characteristics: they are feasible, valuable, and usable (from INSPIRED by Marty Cagan)
- product thinking to internal technology begins with establishing empathy with internal consumers (i.e., fellow developers), collaborating with them on designing the experience, gathering usage metrics, and continuously improving the internal technical solutions over time to maintain ease of use (31)

**baseline usability attributes of data product**
#### discoverable
{: .no_toc }
data product itself intentionally provides discoverability information

#### addressable
{: .no_toc }
provides a permanent and unique address to data user to programmatically or manually access

#### understandable
{: .no_toc }
data product provides semantically coherent data with specific meaning

#### trustworthy & truthful
{: .no_toc }
data product represents facts of business properly; need to know how closely data reflects reality of events that have happened
SLOs:
- **Interval of change**: How often changes in the data are reflected
- **Timeliness**: The skew between the time that a business fact occurs and becomes available to the data users
- **Completeness**: Degree of availability of all the necessary information
- **Statistical shape of data**: Its distribution, range, volume, etc.
- **Lineage**: The data transformation journey from source to here
- **Precision and accuracy over time**: Degree of business truthfulness as time passes
- **Operational qualities**: Freshness, general availability, performance

#### natively accessible
{: .no_toc }
- a data product needs to make it possible for various data users to access and read its data in their native mode of access

#### interoperable
{: .no_toc }
- The key for an effective composability of data across domains is following standards and harmonization rules that allow linking data across domains

**Some things to standardize**
- **Field type**:  A common explicitly defined type system
- **Polysemes identifiers**: Universally identifying entities that cross boundaries of data products
- **Data product global addresses**: A unique global address allocated to each data product, ideally with a uniform scheme for ease of establishing connections to different data products
- **Common metadata fields**: Such as representation of time when data occurs and when data is recorded
- **Schema linking**: Ability to link and reuse schemas—types—defined by other data products
- **Data linking**: Ability to link or map to data in other data products
- **Schema stability**: Approach to evolving schemas that respects backward compatibility

#### valuable on its own
{: .no_toc }
- data product should be valuable without needing to be correlated or joined with other data products

#### secure
{: .no_toc }
- Data users access a data product securely and in a confidentiality-respecting manner. Data security is a must.

- **Access control**: Who, what, and how data users—systems and people—can access the data product
- **Encryption**: What kinds of encryption—on disk, in memory, or in transit—using which encryption algorithm, and how to manage keys and minimize the radius of impact in case of breaches
- **Confidentiality** levels: What kinds of confidential information, e.g., personally identifiable information, personal health information, etc., the data product carries
- **Data retention**: How long the information must be kept
- **Regulations and agreements**: GDPR, CCPA, domain-specific regulations, contractual agreements

two new roles needed for data mesh
1. **data product developer**: responsible for developing, serving, and maintaining domain data products
2. **data product owner**: accountable for success of domain's data products in delivering value, growing data users, and maintaining life cycle of data products

- reframe receiving upstream data from _ingestion_ to _consumption_ -- data is served already cleaned and processed
- success is measured through the value delivered to the users and not its size

### Federated Computational Governance
### Self-Serve Data Platform

Interplay of Four Principles
<div style="text-align:center">
  <a href="/assets/img/data-mesh-principle-interplay.jpg">
    <img src="/assets/img/data-mesh-principle-interplay.jpg" alt="Principle Interplay">
  </a>
</div>

Data Mesh Model
<div style="text-align:center">
  <a href="/assets/img/data-mesh-model.jpg">
    <img src="/assets/img/data-mesh-model.jpg" alt="Data Mesh Model">
  </a>
</div>

- data mesh focuses on analytical data, as opposed to operational data
- operational data is "data on the inside", private data of application