---
last_modified_date: "2022-07-22 19:40:54.029576"
nav_order: 0
nav_exclude: true
parent: Tech Books
book_title: "Data Mesh: Delivering Data-Driven Value at Scale"
author: "Zhamak Dehghani"
publication_year: 2022
---
# Data Mesh: Delivering Data-Driven Value at Scale
{: .no_toc }

<details open markdown="block">
  <summary>
    Table of contents
  </summary>
1. TOC
{:toc}
</details>


## What is Data Mesh?
- Data mesh is a decentralized sociotechnical approach to share, access, and manage analytical data in complex and large-scale environments—within or across organizations
- data mesh looks to achieve these outcomes:
  - Respond gracefully to change: a business’s essential complexity, volatility, and uncertainty
  - Sustain agility in the face of growth
  - Increase the ratio of value from data to investment1

organizational shifts required:
<div style="text-align:center">
  <a href="/assets/img/data-mesh/shifts.jpg">
    <img src="/assets/img/data-mesh/shifts.jpg" alt="Shifts required">
  </a>
</div>

Data Mesh Model
<div style="text-align:center">
  <a href="/assets/img/data-mesh/model.jpg">
    <img src="/assets/img/data-mesh/model.jpg" alt="Data Mesh Model">
  </a>
</div>

- data mesh focuses on analytical data, as opposed to operational data
- operational data is "data on the inside", private data of application


## Data Principles

Interplay of Four Principles
<div style="text-align:center">
  <a href="/assets/img/data-mesh/-principle-interplay.jpg">
    <img src="/assets/img/data-mesh/principle-interplay.jpg" alt="Principle Interplay">
  </a>
</div>

### Domain-Oriented Ownership
- underpinned by domain-driven design to find seams of organizational units to decompose data
- **domain** is a sphere of knowledge, influence, or activity
- **bounded context** - delimited applicability of a particular model that gives team members a clear and shared understanding of what has to be consistent and what can develop independently
- **context mapping** - define relationship between bounded contexts/independent models

three types of domain data
1. **Source-aligned domain data**
  - Analytical data reflecting the business facts generated by the operational systems (native data product)
2. **Aggregate domain data**
  - Analytical data that is an aggregate of multiple upstream domains.
3. **Consumer-aligned domain data**
  - Analytical data transformed to fit the needs of one or multiple specific use cases (fit-for-purpose domain data)

{: .warn }
> Exposing analytical data directly from the operational database is an antipattern

- work with multiple models of shared entities rather than single source of truth, which is expensive and impediment to scale
- In data mesh, a data pipeline is simply an internal implementation of the data domain and is handled internally within the domain. It’s an implementation detail that must be abstracted from outside of the domain (26)

### Data as a Product

- successful products have three common characteristics: they are feasible, valuable, and usable (from INSPIRED by Marty Cagan)
- product thinking to internal technology begins with establishing empathy with internal consumers (i.e., fellow developers), collaborating with them on designing the experience, gathering usage metrics, and continuously improving the internal technical solutions over time to maintain ease of use (31)

**baseline usability attributes of data product**
#### discoverable
{: .no_toc }
data product itself intentionally provides discoverability information

#### addressable
{: .no_toc }
provides a permanent and unique address to data user to programmatically or manually access

#### understandable
{: .no_toc }
data product provides semantically coherent data with specific meaning

#### trustworthy & truthful
{: .no_toc }
data product represents facts of business properly; need to know how closely data reflects reality of events that have happened
SLOs:
- **Interval of change**: How often changes in the data are reflected
- **Timeliness**: The skew between the time that a business fact occurs and becomes available to the data users
- **Completeness**: Degree of availability of all the necessary information
- **Statistical shape of data**: Its distribution, range, volume, etc.
- **Lineage**: The data transformation journey from source to here
- **Precision and accuracy over time**: Degree of business truthfulness as time passes
- **Operational qualities**: Freshness, general availability, performance

#### natively accessible
{: .no_toc }
- a data product needs to make it possible for various data users to access and read its data in their native mode of access

#### interoperable
{: .no_toc }
- The key for an effective composability of data across domains is following standards and harmonization rules that allow linking data across domains

Some things to standardize
- **Field type**:  A common explicitly defined type system
- **Polysemes identifiers**: Universally identifying entities that cross boundaries of data products
- **Data product global addresses**: A unique global address allocated to each data product, ideally with a uniform scheme for ease of establishing connections to different data products
- **Common metadata fields**: Such as representation of time when data occurs and when data is recorded
- **Schema linking**: Ability to link and reuse schemas—types—defined by other data products
- **Data linking**: Ability to link or map to data in other data products
- **Schema stability**: Approach to evolving schemas that respects backward compatibility

#### valuable on its own
{: .no_toc }
- data product should be valuable without needing to be correlated or joined with other data products

#### secure
{: .no_toc }
- Data users access a data product securely and in a confidentiality-respecting manner. Data security is a must.

- **Access control**: Who, what, and how data users—systems and people—can access the data product
- **Encryption**: What kinds of encryption—on disk, in memory, or in transit—using which encryption algorithm, and how to manage keys and minimize the radius of impact in case of breaches
- **Confidentiality** levels: What kinds of confidential information, e.g., personally identifiable information, personal health information, etc., the data product carries
- **Data retention**: How long the information must be kept
- **Regulations and agreements**: GDPR, CCPA, domain-specific regulations, contractual agreements

two new roles needed for data mesh
1. **data product developer**: responsible for developing, serving, and maintaining domain data products
2. **data product owner**: accountable for success of domain's data products in delivering value, growing data users, and maintaining life cycle of data products

- reframe receiving upstream data from _ingestion_ to _consumption_ -- data is served already cleaned and processed
- success is measured through the value delivered to the users and not its size

### Self-Serve Data Platform
- data platform team to provide generic data platform capabilities that allow scale out, sharing, accessing, and using analytical data in a _decentralized_ manner

> The main responsibility of the data mesh platform is to enable existing or new domain engineering teams with the new and embedded responsibilities of building, sharing, and using data products end to end; capturing data from operational systems and other sources; and transforming and sharing the data as a product with the end data users.

- _platform_ is used as shorthand for set of underlying data infrastructure capabilities -- doesn't mean single solution or single vendor

<div style="text-align:center">
  <a href="/assets/img/data-mesh/platform.jpg">
    <img src="/assets/img/data-mesh/platform.jpg" alt="Data Mesh Platform">
  </a>
</div>


shift how we talk about data platform
<div style="text-align:center">
  <a href="/assets/img/data-mesh/platform-distinguishing.jpg">
    <img src="/assets/img/data-mesh/platform-distinguishing.jpg" alt="Data Mesh Data Platform Distinguishing Characteristics">
  </a>
</div>

- accommodate generalists (T shape, paint drip) rather than current industry requirement of specialists

key downsides of current data platform thinking
  - cost is estimated and managed monolithically, not per isolated domain resources
  - security and privacy management assume physical resources are shared under same account and don't scale to isolated security context per data product
  - A central pipeline (DAG) orchestration assumes management of all data pipelines centrally, which conflicts with independent pipeline

data platform objectives:
1. **enable autonomous teams to get value from data** - platform enables product developer to concentrate on domain-specific aspects of data product development
2. **exchange value with autonomous and interoperable data products** - mesh becomes organizational data marketplace
3. **accelerate exchange of value by lowering the cognitive load** - abstracting complexity; should be able to _declare_ structure of data, retention period, potential size, etc. -- remove human intervention and manual steps
4. **scale out data sharing** - embrace Unix philosophy
5. **support a culture of embedded innovation** - rapidly building, testing, and refining ideas; need to free people from unnecessary work and accidental complexity

how to transition to self-servce data mesh platform
- **design APIs and protocols first** - start with interfaces platform exposes to its users
- **prepare for a generalist approach** - favor platform tech that fits better with natural style of programming (not something that creates another DSL)
- **do an inventory and simplify** - take a look at what you already have
- **create higher-level APIs to manage data products** - platform must introduce higher level of APIs that deal with data product as an object
- **build experiences, not mechanisms** - shift articulation of platform from mechanisms it includes to the experiences it enables
- **begin with simplest foundation, then harvest to evolve** - data platform is emergent, designed and brought to life incrementally by many

### Federated Computational Governance
> **Federated and computational governance** is a decision-making model led by the federation of domain data product owners and data platform product owners, with autonomy and domain-local decision-making power, while creating and adhering to a set of global rules

<div style="text-align:center">
  <a href="/assets/img/data-mesh/elements-of-governance.jpg">
    <img src="/assets/img/data-mesh/elements-of-governance.jpg" alt="Elements of Data Mesh Federated Governance">
  </a>
</div>

#### Apply Systems Thinking to Data Mesh Governance
{: .no_toc }
- data mesh requires a governance model that embraces systems thinking. Systems thinking, as described by Peter Senge, is the discipline of “seeing the whole,” shifting our focus “from parts
to the organization of parts, recognizing interaction of the parts are not static and constant, but dynamic processes.”
- mesh is more than the sum of its parts
- the art of governing a data mesh ecosystem is in maintaining an equilibrium between local (domain) optimization and global (the mesh) optimization
- need to continuously adjust behavior using _leverage points_ (small change lead to large shift in behavior) and _feedback loops_ (system structures that either balance or reinforce a change in state of system)
- embrace dynamic topology as default state
- utilize automation and distributed architecture

#### Apply Federation to the Governance Model
{: .no_toc }
**Federated Team** - team that decides policies implemented, how platform supports these policies computationally, how data products adopt policies
- created of domain representatives, data platform representatives, subject matter experts, and facilitators and managers

**Guiding Values** - clarity on value system that guides how decisions are made
- localize decisions and responsibility close to the source
- identify cross-cutting concerns that need a global standard
- globalize decisions that facilitate interoperability
- identify consistent experiences that need a global standard
- execute decisions locally

**Policies** - output of a system governance, between local and global effect

**Incentives** - what incentives to use to motivate as leverage points, can be local or global

#### Apply Computation to the Governance Model
{: .no_toc }

**Standards as Code** behavior, interfaces, and data structure that is expected to be implemented in a consistent way across all data products
- data product discovery and observability interfaces
- data product data interfaces
- data and query modeling language
- lineage modeling
- polysemes identification modeling

**Policies as Code**
- data privacy and protection
- data localization
- data access control and audit
- data consent
- data sovereignty
- data retention

**Automated Tests**
**Automated Monitoring**

> data mesh governance heavily relies on embedding the governance policies into each data product in an automated and computational fashion. This of course heavily relies on the elements of the underlying data platform, to make it really easy to do the right thing.

> The continuous need for trustworthy and useful data across multiple domains to train ML-based solutions will be the ultimate motivator for the adoption of data mesh governance and doing the right thing.

## Why Data Mesh?
- **great expectations of data** - diverse and wide applications of ML and analytics
- **great divide of data** - complexity risen from fragmentation of operational and analytical data
- **scale** - large scale data source proliferation
- **business complexity and volatility** - continuous change and growth of business
- **discord between data investments and returns** - expensive data solutions lacking impact

> Data mesh learns from the past solutions and addresses their shortcomings. It reduces points of centralization that act as coordination bottlenecks. It finds a new way of decomposing the data architecture without slowing the organization down with synchronizations. It removes the gap between where the data originates and where it gets used and removes the accidental complexities—aka pipelines—that happen in between the two planes of data. Data mesh departs from data myths such as a single source of truth, or one tightly controlled canonical data model.

<div style="text-align:center">
  <a href="/assets/img/data-mesh/after-the-inflection.jpg">
    <img src="/assets/img/data-mesh/after-the-inflection.jpg" alt="Data Mesh, After the Inflection Point">
  </a>
</div>

## Progression of Data Architecture
### First Generation: Data Warehouse Architecture
{: .no_toc }
- Extracted from many operational databases and sources
- Transformed into a universal schema—represented in a multidimensional and time-variant tabular format
- Loaded into the warehouse tables
- Accessed through SQL-like queries
- Mainly serving data analysts for reporting and analytical visualization use cases

### Second Generation: Data Lake Architecture
{: .no_toc }
- introduced in 2010 as response to challenge of data warehouse in satisfying data scientists who wanted data in original format
- Data is extracted from many operational databases and sources.
- Data represents as much as possible of the original content and structure.
- Data is minimally transformed to fit the popular storage formats, e.g., Parquet, Avro, etc.
- Data—as close as possible to the source schema—is loaded to scalable object storage.
- Data is accessed through the object storage interface—read as files or data frames, a two-dimensional array-like structure.
- Data scientists mainly access the lake storage for analytical and machine learning model training.
- Downstream from the lake, lakeshore marts are created as fit-for-purpose data marts.
- Lakeshore marts are used by applications and analytics use cases.
- Downstream from the lake, feature stores are created as fit-for-purpose columnar data modeled and stored for machine learning training.

### Third Generation: Multimodel Cloud Architecture
{: .no_toc }
- Support streaming for near real-time data availability with architectures such as Kappa.
- Attempt to unify batch and stream processing for data transformation with frameworks such as Apache Beam.
- Fully embrace cloud-based managed services and use modern cloud-native implementations with isolated compute and storage. They leverage the elasticity of the cloud for cost optimization.
- Converge the warehouse and lake into one technology, either extending the data warehouse to include embedded ML training, or alternatively building data warehouse integrity, transactionality, and querying systems into data lake solutions.

### Characteristics of Current Data Architecture
{: .no_toc }
- monolithic architecture - 1) ingest data, 2) cleanse enrich and transform data, and 3) serve data
- monolithic technology
- monolithic organization - central data team, conway's law
- centrally owned
- technically partitioned (rather than domain partitioned) -- partitioning is orthogonal to axes of change and slows releases

## How to Design Data Mesh
